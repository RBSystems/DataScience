{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up data, convert to dataframes, and save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BAC file is pipe separated with the following fields:\n",
    "LogLevel [TimeStamp]:[LogVisibility][LogSeverity][entryType][entrySubType][eventType][EventType dependent strings]\n",
    "\n",
    "The information changes at the end of each entry based on its EventType.  Here is a list of the supported event types and the subsequent additional information for each listed below.\n",
    "\n",
    "GeneralMessage - [string message]\n",
    "\n",
    "LevelChangedEvent - [load ID][loadName][roomName][rampTime][rampBaseValue][rampFinalValue]\n",
    "\n",
    "ButtonChangedEvent - [keypad ID][keypadName][roomName][buttonNum][buttonState]\n",
    "\n",
    "RemoteSystemEvent - [signalID][signalName][roomName][RemoteSystemEvent string]\n",
    "TimeClockChangedEvent/OccupancyChangedEvent/SceneChangedEvent - [ID][name][roomName][message]\n",
    "\n",
    "ConnectionStatus - [device ID][Name][room Name][connection status][Load 1 Room Name:Load 1 Name]|[Load 2 Room Name:Load 2 Name]\n",
    "\n",
    "    NOTE: DeviceConnectionStatusWithOptions is the same format as ConnectionStatus. \n",
    "    \n",
    "SignalChangedEvent - [device ID][Device Name][room Name][signal event ID][signal Value] - Signal event ID differs by device and signal value is either bool or int based on the eventID.\n",
    "\n",
    "SignalChangedEventWithStrings - [device ID][Device Name][Signal Name][Signal Value string][Signal direction][message]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:40:25.643210Z",
     "start_time": "2018-06-11T18:40:24.827769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanrahman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "from numpy import nan\n",
    "from datetime import datetime, timedelta, date\n",
    "from fractions import Fraction\n",
    "from time import mktime\n",
    "import time\n",
    "import requests\n",
    "import os.path\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:40:25.651296Z",
     "start_time": "2018-06-11T18:40:25.645376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_or_load_from_checkpoint(checkpoint_name):\n",
    "    if os.path.isfile(checkpoint_name):\n",
    "        return pd.read_hdf(checkpoint_name,'table')\n",
    "    df.to_hdf(checkpoint_name, 'table', mode='w', append=True, complevel=9, complib='zlib', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T21:03:45.111269Z",
     "start_time": "2018-06-08T21:03:44.906930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['LogLevel',\n",
    "          'TimeStamp',\n",
    "          'LogVisibility',\n",
    "          'LogSeverity',\n",
    "          'entryType',\n",
    "          'entrySubType',\n",
    "          'eventType']\n",
    "# from label list\n",
    "EVENT_TYPE_INDEX = labels.index('eventType')\n",
    "\n",
    "# from line in .bac file\n",
    "LOG_LEVEL_START = 0\n",
    "LOG_LEVEL_END = 3\n",
    "TIMESTAMP_START = 6\n",
    "TIMESTAMP_END = 14\n",
    "PIPE_SEPARATED_DATA_START = 17\n",
    "\n",
    "event_type_labels = [\n",
    "    \"string message\", # GeneralMessage\n",
    "    \"load ID\", \"loadName\", \"roomName1\", \"rampTime\", \"rampBaseValue\", \"rampFinalValue\", # LevelChangedEvent\n",
    "    \"keypad ID\", \"keypadName\", \"roomName2\", \"buttonNum\", \"buttonState\", # ButtonChangedEvent\n",
    "    \"signalID\", \"signalName\", \"roomName3\", \"RemoteSystemEvent string\", # RemoteSystemEvent\n",
    "    \"ID\", \"name\", \"roomName4\", \"message1\", # TimeClockChangedEvent/OccupancyChangedEvent/SceneChangedEvent\n",
    "    \"device ID1\", \"Name\", \"roomName5\", \"connection status\", \"Load 1 Room Name:Load 1 Name\", \"Load 2 Room Name:Load 2 Name\", # ConnectionStatus/DeviceConnectionStatusWithOptions\n",
    "    \"device ID2\", \"Device Name1\", \"roomName6\", \"signal event ID\", \"signal Value\", # SignalChangedEvent\n",
    "    \"device ID3\", \"Device Name2\", \"Signal Name\", \"Signal Value string\", \"Signal direction\", \"message2\", # SignalChangedEventWithStrings\n",
    "]\n",
    "\n",
    "def from_bac():\n",
    "    # device_id_offsets = [i for i, x in enumerate(event_type_labels) if x == \"device ID\"]\n",
    "    clean_lines = []\n",
    "\n",
    "    for log in os.listdir('data'):\n",
    "        with open('data/{}'.format(log)) as logfile:\n",
    "            for line in logfile:\n",
    "                line = line.rstrip('\\n')\n",
    "                if line[-1] == '|':\n",
    "                    line = line[:-1]\n",
    "                all_data = ([line[LOG_LEVEL_START:LOG_LEVEL_END], line[TIMESTAMP_START:TIMESTAMP_END]]\n",
    "                            + line[PIPE_SEPARATED_DATA_START:].split('|'))\n",
    "\n",
    "                event_type_dependent_strings = all_data[len(labels):]\n",
    "                clean_line = all_data[:len(labels)]\n",
    "\n",
    "                START_INDEX = 7\n",
    "                if clean_line[EVENT_TYPE_INDEX] == 'GeneralMessage':\n",
    "                    START_INDEX += event_type_labels.index('string message')\n",
    "                    # account for pipes in the message string\n",
    "                    event_type_dependent_strings = ['|'.join(event_type_dependent_strings)]\n",
    "                    assert START_INDEX == 7\n",
    "                    assert len(event_type_dependent_strings) == 1, event_type_dependent_strings\n",
    "\n",
    "                elif clean_line[EVENT_TYPE_INDEX] == 'LevelChangedEvent':\n",
    "                    START_INDEX += event_type_labels.index('load ID')\n",
    "                    assert START_INDEX == 8\n",
    "                    assert len(event_type_dependent_strings) == 6\n",
    "\n",
    "                elif clean_line[EVENT_TYPE_INDEX] == 'ButtonChangedEvent':\n",
    "                    START_INDEX += event_type_labels.index('keypad ID')\n",
    "                    assert START_INDEX == 14\n",
    "                    assert len(event_type_dependent_strings) == 5\n",
    "\n",
    "                elif clean_line[EVENT_TYPE_INDEX] == 'RemoteSystemEvent':\n",
    "                    START_INDEX += event_type_labels.index('signalID')\n",
    "                    assert START_INDEX == 19\n",
    "                    assert len(event_type_dependent_strings) == 4\n",
    "\n",
    "                elif (clean_line[EVENT_TYPE_INDEX] == 'TimeClockChangedEvent' or\n",
    "                      clean_line[EVENT_TYPE_INDEX] == 'OccupancyChangedEvent' or\n",
    "                      clean_line[EVENT_TYPE_INDEX] == 'SceneChangedEvent'):\n",
    "                    START_INDEX += event_type_labels.index('ID')\n",
    "                    assert START_INDEX == 23\n",
    "                    assert len(event_type_dependent_strings) == 4\n",
    "\n",
    "                elif (clean_line[EVENT_TYPE_INDEX] == 'ConnectionStatus' or\n",
    "                      clean_line[EVENT_TYPE_INDEX] == 'DeviceConnectionStatusWithOptions'):\n",
    "                    START_INDEX += event_type_labels.index('device ID1')\n",
    "                    assert START_INDEX == 27\n",
    "                    assert (len(event_type_dependent_strings) == 4 or\n",
    "                            len(event_type_dependent_strings) == 5 or\n",
    "                            len(event_type_dependent_strings) == 6)\n",
    "\n",
    "                elif clean_line[EVENT_TYPE_INDEX] == 'SignalChangedEvent':\n",
    "                    START_INDEX += event_type_labels.index('device ID2')\n",
    "                    assert START_INDEX == 33\n",
    "                    assert len(event_type_dependent_strings) == 5\n",
    "\n",
    "                elif clean_line[EVENT_TYPE_INDEX] == 'SignalChangedEventWithStrings':\n",
    "                    START_INDEX += event_type_labels.index('device ID3')\n",
    "                    assert START_INDEX == 38\n",
    "                    # to correct for the double pipe in \"Basement Mudroom\"\n",
    "                    event_type_dependent_strings = [i for i in event_type_dependent_strings if i]\n",
    "                else:\n",
    "                    raise ValueError(\"Wrong event type: {}\".format(clean_line[EVENT_TYPE_INDEX]))\n",
    "\n",
    "                clean_line = clean_line + [np.nan]*len(event_type_labels) + [log[-14:-4]]\n",
    "                clean_line[START_INDEX:START_INDEX + len(event_type_dependent_strings)] = event_type_dependent_strings\n",
    "                clean_lines.append(clean_line)\n",
    "    df = pd.DataFrame(clean_lines, columns=labels + event_type_labels + [\"date\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from BAC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:23:00.578019Z",
     "start_time": "2018-06-08T20:22:45.059544Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = from_bac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:23:00.586871Z",
     "start_time": "2018-06-08T20:23:00.580257Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_datetime(row):\n",
    "    new_date = row['date'] + ' ' + row['TimeStamp']\n",
    "    dt = datetime.strptime(new_date, '%Y-%m-%d %H:%M:%S')\n",
    "    dt = dt + timedelta(hours=4)\n",
    "    unix_secs = mktime(dt.timetuple())\n",
    "    return unix_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:23:58.951447Z",
     "start_time": "2018-06-08T20:23:00.590325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['unix_time'] = df.apply(to_datetime, axis=1)\n",
    "df = df.drop([\"TimeStamp\",\"date\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:24:02.816549Z",
     "start_time": "2018-06-08T20:23:58.954004Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useless columns\n",
    "df = df.drop([\"LogLevel\", \"LogVisibility\", \"LogSeverity\"],axis=1, errors='ignore')\n",
    "\n",
    "# useless after deleting General Message\n",
    "df = df.drop(\"string message\", axis=1, errors='ignore')\n",
    "\n",
    "# useless after deleting Button Change Event\n",
    "df = df.drop([\"keypad ID\", \"keypadName\", \"roomName2\", \"buttonNum\", \"buttonState\"], axis=1, errors='ignore')\n",
    "\n",
    "# useless after deleting Connection Status\n",
    "df = df.drop([\"device ID1\", \"Name\", \"roomName5\", \"connection status\", \"Load 1 Room Name:Load 1 Name\", \"Load 2 Room Name:Load 2 Name\"], axis=1, errors='ignore')\n",
    "\n",
    "# useless after deleting Remote System Event\n",
    "df = df.drop([\"signalID\", \"signalName\", \"roomName3\", \"RemoteSystemEvent string\"], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove useless event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:24:06.662246Z",
     "start_time": "2018-06-08T20:24:02.819377Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['eventType'] != 'ButtonChangedEvent']\n",
    "df = df[df['eventType'] != 'GeneralMessage']\n",
    "df = df[df['eventType'] != 'RemoteSystemEvent']\n",
    "df = df[df['eventType'] != 'TimeClockChangedEvent']\n",
    "df = df[df['eventType'] != 'ConnectionStatus']\n",
    "df = df[df['eventType'] != 'DeviceConnectionStatusWithOptions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Ids and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:18.212363Z",
     "start_time": "2018-06-08T20:30:15.653329Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge room names\n",
    "df['room_name_merged'] = df['roomName1'].fillna('') + df['roomName4'].fillna('') + df['roomName6'].fillna('')\n",
    "df = df.drop([\"roomName1\",\"roomName4\",\"roomName6\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:20.351864Z",
     "start_time": "2018-06-08T20:30:18.216275Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge device name\n",
    "df['device_name'] = df['loadName'].fillna('') + df['name'].fillna('') + df['Device Name1'].fillna('') + df['Device Name2'].fillna('')\n",
    "df = df.drop([\"loadName\", \"name\", \"Device Name1\", \"Device Name2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:22.243510Z",
     "start_time": "2018-06-08T20:30:20.354830Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge IDs \n",
    "df['device_id'] = df['load ID'].fillna('') + df['ID'].fillna('') + df['device ID2'].fillna('') + df['device ID3'].fillna('')\n",
    "df = df.drop(['load ID', 'ID', 'device ID2', 'device ID3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:23.600670Z",
     "start_time": "2018-06-08T20:30:22.247009Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge messages \n",
    "df['message_merged'] = df['message1'].fillna('') + df['message2'].fillna('')\n",
    "df = df.drop([\"message1\", \"message2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:25.412362Z",
     "start_time": "2018-06-08T20:30:23.603498Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.replace('', np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine device name and id to for truly unique ids\n",
    "\n",
    "The devices had no unique identifier stored. We created unique identifiers for each device by combining device_name and device_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:32.390998Z",
     "start_time": "2018-06-08T20:30:31.976083Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['device_id_name'] = df['device_id'].astype(str) + df['device_name'].astype(str)\n",
    "# df = df.drop([\"device_id\", \"device_name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:30:36.931414Z",
     "start_time": "2018-06-08T20:30:33.580647Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:31:46.539007Z",
     "start_time": "2018-06-08T20:30:43.516452Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checkpoint \n",
    "df = save_or_load_from_checkpoint('./checkpoints/data_0.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:31:47.381761Z",
     "start_time": "2018-06-08T20:31:46.541172Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropping more useless data\n",
    "index_to_drop = df.loc[df[\"entryType\"] == \"Auxiliary\"][df['eventType'] == \"SignalChangedEventWithStrings\"].index.tolist()\n",
    "df = df.drop(index_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:32:29.145887Z",
     "start_time": "2018-06-08T20:32:29.138722Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_setpoints(row):\n",
    "    signal_direction = str(row['Signal direction'])\n",
    "    temp = np.nan\n",
    "    if 'Fahrenheit' in signal_direction and signal_direction[0].isdigit():\n",
    "        temp = int(''.join(x for x in signal_direction if x.isdigit()))\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:32:40.935792Z",
     "start_time": "2018-06-08T20:32:30.423080Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['temperature'] = df.apply(get_setpoints, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:36:46.823052Z",
     "start_time": "2018-06-08T20:36:29.717257Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = save_or_load_from_checkpoint('./checkpoints/data_1.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Binary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:38:12.496229Z",
     "start_time": "2018-06-08T20:38:12.492206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_signal_direction(df, string_val):\n",
    "    return df[df[\"Signal Value string\"] == string_val]['Signal direction'].unique().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:38:15.976776Z",
     "start_time": "2018-06-08T20:38:13.833877Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in sorted(df[\"Signal Value string\"].astype(str).unique()):\n",
    "    d[i] = check_signal_direction(df, i)\n",
    "for k,v in d.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:38:36.551105Z",
     "start_time": "2018-06-08T20:38:36.545410Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'Active Fan Level' : ['High','Off'],\n",
    "'Auto Mode': ['Enabled', 'Disabled'],\n",
    "'Cool Stage 1' : ['Active', 'Inactive'],\n",
    "'Fan' : ['On', 'Auto'],\n",
    "'Floor Warming' : ['Heat:False','Off:False', 'Off:True'],\n",
    "'Heat Stage 1' : ['Active', 'Inactive'],\n",
    "'Hold' : ['On', 'Off'],\n",
    "'Humidifier Off' : ['Inactive', 'Active'],\n",
    "'Mode' : ['Heat' 'Cool' 'Off']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:40:13.741247Z",
     "start_time": "2018-06-08T20:40:13.732457Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_binary(row, **kwargs):\n",
    "    \"\"\"kwargs = [Signal Value, values_dict]\"\"\"\n",
    "    if str(row['eventType']) == 'SignalChangedEventWithStrings':\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for key, values in kwargs.items():\n",
    "            if str(row['Signal Value string']) == key:\n",
    "                for val in values:\n",
    "                    if str(row['Signal direction']) == val:\n",
    "                        return key + val\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:40:30.959683Z",
     "start_time": "2018-06-08T20:40:14.895362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['event'] = df.apply(is_binary, **d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:40:30.970162Z",
     "start_time": "2018-06-08T20:40:30.962391Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_locked_or_occupied(row):\n",
    "    if str(row['eventType']) == 'OccupancyChangedEvent':\n",
    "        return row['message_merged']\n",
    "    elif str(row['Signal Value string']) == 'Lock' or str(row['Signal Value string']) == 'Unlock':\n",
    "        return row['entryType'] + row['Signal Value string']\n",
    "    elif pd.notna(row['event']):\n",
    "        return str(row['event'])\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:41:01.626553Z",
     "start_time": "2018-06-08T20:40:30.974134Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['event'] = df.apply(is_locked_or_occupied, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:41:35.809237Z",
     "start_time": "2018-06-08T20:41:35.780417Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_value(row):\n",
    "    value = np.nan\n",
    "    if pd.notna(row['rampFinalValue']):\n",
    "        value = int(row['rampFinalValue'])\n",
    "    elif pd.notna(row['temperature']):\n",
    "        value = int(row['temperature'])\n",
    "    elif str(row['entryType']) == 'Shades' and str(row['eventType']) == 'SceneChangedEvent':\n",
    "        base_str = str(row['device_id_name'])\n",
    "        split_str = base_str.split(' ')\n",
    "        first_str = split_str[0][-3:]\n",
    "        is_closed = True if split_str[1] == 'Closed' else False\n",
    "        if first_str[-1].isdigit():\n",
    "            frac = float(Fraction(first_str))\n",
    "            if is_closed:\n",
    "                frac = 1 - frac\n",
    "            value = frac\n",
    "        elif is_closed:\n",
    "            value = 0\n",
    "        else:\n",
    "            value = 1\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:41:58.822372Z",
     "start_time": "2018-06-08T20:41:37.488541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['value'] = df.apply(get_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:41:58.832190Z",
     "start_time": "2018-06-08T20:41:58.825695Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_event_type(row): \n",
    "    event_type = np.nan\n",
    "    if pd.notna(row['value']):\n",
    "        event_type = str(row['entryType'])\n",
    "    return event_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T20:42:13.526581Z",
     "start_time": "2018-06-08T20:41:58.836003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['regression_value_type'] = df.apply(get_event_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:42:15.384265Z",
     "start_time": "2018-06-11T18:41:57.180900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = save_or_load_from_checkpoint('./checkpoints/data_2.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:42:15.575763Z",
     "start_time": "2018-06-11T18:42:15.387016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[['device_id_name', 'event', 'regression_value_type', 'value', 'unix_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:42:20.187734Z",
     "start_time": "2018-06-11T18:42:15.578076Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['unix_time'] = pd.to_datetime(df['unix_time'], unit='s')\n",
    "df = df.sort_values(by='unix_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:42:20.215154Z",
     "start_time": "2018-06-11T18:42:20.189996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id_name</th>\n",
       "      <th>event</th>\n",
       "      <th>regression_value_type</th>\n",
       "      <th>value</th>\n",
       "      <th>unix_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>118Sconces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lights</td>\n",
       "      <td>19275.0</td>\n",
       "      <td>2017-08-04 08:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>74Night</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-04 08:00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>208Basement Mudroom Door</td>\n",
       "      <td>DoorLockLock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-04 08:00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>77Lock Basement Mudroom Door</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-04 08:00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>118Sconces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lights</td>\n",
       "      <td>19275.0</td>\n",
       "      <td>2017-08-04 08:00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    device_id_name         event regression_value_type  \\\n",
       "5203                    118Sconces           NaN                Lights   \n",
       "5193                       74Night           NaN                   NaN   \n",
       "5195      208Basement Mudroom Door  DoorLockLock                   NaN   \n",
       "5196  77Lock Basement Mudroom Door           NaN                   NaN   \n",
       "5206                    118Sconces           NaN                Lights   \n",
       "\n",
       "        value           unix_time  \n",
       "5203  19275.0 2017-08-04 08:00:06  \n",
       "5193      NaN 2017-08-04 08:00:07  \n",
       "5195      NaN 2017-08-04 08:00:08  \n",
       "5196      NaN 2017-08-04 08:00:08  \n",
       "5206  19275.0 2017-08-04 08:00:09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:50:00.342447Z",
     "start_time": "2018-06-11T18:50:00.201117Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[(df['event'].notnull()) | (df['regression_value_type'].notnull()) | (df['value'].notnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add week, day, month, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T18:50:07.932050Z",
     "start_time": "2018-06-11T18:50:07.751015Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_dates(data_frame):\n",
    "    date = pd.to_datetime(data_frame['unix_time'], unit='s')\n",
    "    data_frame['week'] = date.dt.week\n",
    "    data_frame['day'] = date.dt.day\n",
    "    data_frame['month'] = date.dt.month\n",
    "    data_frame['hour'] = date.dt.hour\n",
    "    data_frame['minute'] = date.dt.minute\n",
    "    data_frame['second'] = date.dt.second\n",
    "    return data_frame\n",
    "\n",
    "df = add_dates(df)\n",
    "# df = df.drop('unix_time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:06:15.691281Z",
     "start_time": "2018-06-11T19:06:14.214704Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = df['unix_time'].map(pd.Timestamp.date).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T19:51:30.237019Z",
     "start_time": "2018-06-11T19:51:30.231383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2017, 8, 4), datetime.date(2017, 8, 5),\n",
       "       datetime.date(2017, 8, 6), datetime.date(2017, 8, 7),\n",
       "       datetime.date(2017, 8, 8), datetime.date(2017, 8, 9),\n",
       "       datetime.date(2017, 8, 10), datetime.date(2017, 8, 11),\n",
       "       datetime.date(2017, 8, 12), datetime.date(2017, 8, 13),\n",
       "       datetime.date(2017, 8, 14), datetime.date(2017, 8, 15),\n",
       "       datetime.date(2017, 8, 16), datetime.date(2017, 8, 17),\n",
       "       datetime.date(2017, 8, 18), datetime.date(2017, 8, 19),\n",
       "       datetime.date(2017, 8, 20), datetime.date(2017, 8, 21),\n",
       "       datetime.date(2017, 8, 22), datetime.date(2017, 8, 23),\n",
       "       datetime.date(2017, 8, 24), datetime.date(2017, 8, 25),\n",
       "       datetime.date(2017, 8, 26), datetime.date(2017, 8, 27),\n",
       "       datetime.date(2017, 8, 28), datetime.date(2017, 8, 29),\n",
       "       datetime.date(2017, 8, 30), datetime.date(2017, 8, 31),\n",
       "       datetime.date(2017, 9, 1), datetime.date(2017, 9, 2),\n",
       "       datetime.date(2017, 9, 3), datetime.date(2017, 9, 4),\n",
       "       datetime.date(2017, 9, 5), datetime.date(2017, 9, 7),\n",
       "       datetime.date(2017, 9, 8), datetime.date(2017, 9, 9),\n",
       "       datetime.date(2017, 9, 10), datetime.date(2017, 9, 11),\n",
       "       datetime.date(2017, 9, 12), datetime.date(2017, 9, 13),\n",
       "       datetime.date(2017, 9, 14), datetime.date(2017, 9, 15),\n",
       "       datetime.date(2017, 9, 16), datetime.date(2017, 9, 17),\n",
       "       datetime.date(2017, 9, 18), datetime.date(2017, 9, 19),\n",
       "       datetime.date(2017, 9, 20), datetime.date(2017, 9, 21),\n",
       "       datetime.date(2017, 9, 22), datetime.date(2017, 9, 23),\n",
       "       datetime.date(2017, 9, 24), datetime.date(2017, 9, 25),\n",
       "       datetime.date(2017, 9, 26), datetime.date(2017, 9, 27),\n",
       "       datetime.date(2017, 9, 28), datetime.date(2017, 9, 29),\n",
       "       datetime.date(2017, 9, 30), datetime.date(2017, 10, 1),\n",
       "       datetime.date(2017, 10, 2), datetime.date(2017, 10, 3),\n",
       "       datetime.date(2017, 10, 4), datetime.date(2017, 10, 5),\n",
       "       datetime.date(2017, 10, 6), datetime.date(2017, 10, 7),\n",
       "       datetime.date(2017, 10, 8), datetime.date(2017, 10, 9),\n",
       "       datetime.date(2017, 10, 10), datetime.date(2017, 10, 11),\n",
       "       datetime.date(2017, 10, 12), datetime.date(2017, 10, 13),\n",
       "       datetime.date(2017, 10, 14), datetime.date(2017, 10, 15),\n",
       "       datetime.date(2017, 10, 16), datetime.date(2017, 10, 17),\n",
       "       datetime.date(2017, 10, 18), datetime.date(2017, 10, 19),\n",
       "       datetime.date(2017, 10, 20), datetime.date(2017, 10, 21),\n",
       "       datetime.date(2017, 10, 22), datetime.date(2017, 10, 23),\n",
       "       datetime.date(2017, 10, 24), datetime.date(2017, 10, 26),\n",
       "       datetime.date(2017, 10, 27), datetime.date(2017, 10, 28),\n",
       "       datetime.date(2017, 10, 29), datetime.date(2017, 10, 30),\n",
       "       datetime.date(2017, 10, 31), datetime.date(2017, 11, 1),\n",
       "       datetime.date(2017, 11, 2), datetime.date(2017, 11, 3),\n",
       "       datetime.date(2017, 11, 4), datetime.date(2017, 11, 5),\n",
       "       datetime.date(2017, 11, 6), datetime.date(2017, 11, 7),\n",
       "       datetime.date(2017, 11, 8), datetime.date(2017, 11, 9),\n",
       "       datetime.date(2017, 11, 10), datetime.date(2017, 11, 11),\n",
       "       datetime.date(2017, 11, 12), datetime.date(2017, 11, 13),\n",
       "       datetime.date(2017, 11, 14), datetime.date(2017, 11, 15),\n",
       "       datetime.date(2017, 11, 16), datetime.date(2017, 11, 17),\n",
       "       datetime.date(2017, 11, 18), datetime.date(2017, 11, 19),\n",
       "       datetime.date(2017, 11, 20), datetime.date(2017, 11, 21),\n",
       "       datetime.date(2017, 11, 22), datetime.date(2017, 11, 23),\n",
       "       datetime.date(2017, 11, 24), datetime.date(2017, 11, 25),\n",
       "       datetime.date(2017, 11, 26), datetime.date(2017, 11, 27),\n",
       "       datetime.date(2017, 11, 28), datetime.date(2017, 11, 29),\n",
       "       datetime.date(2017, 11, 30), datetime.date(2017, 12, 1),\n",
       "       datetime.date(2017, 12, 2), datetime.date(2017, 12, 3),\n",
       "       datetime.date(2017, 12, 4), datetime.date(2017, 12, 5),\n",
       "       datetime.date(2017, 12, 6), datetime.date(2017, 12, 7),\n",
       "       datetime.date(2017, 12, 8), datetime.date(2017, 12, 9),\n",
       "       datetime.date(2017, 12, 10), datetime.date(2017, 12, 11),\n",
       "       datetime.date(2017, 12, 12), datetime.date(2017, 12, 13),\n",
       "       datetime.date(2017, 12, 14), datetime.date(2017, 12, 15),\n",
       "       datetime.date(2017, 12, 16), datetime.date(2017, 12, 17),\n",
       "       datetime.date(2017, 12, 18), datetime.date(2017, 12, 19),\n",
       "       datetime.date(2017, 12, 20), datetime.date(2017, 12, 21),\n",
       "       datetime.date(2017, 12, 22), datetime.date(2017, 12, 23),\n",
       "       datetime.date(2017, 12, 24), datetime.date(2017, 12, 25),\n",
       "       datetime.date(2017, 12, 26), datetime.date(2017, 12, 27),\n",
       "       datetime.date(2017, 12, 28), datetime.date(2017, 12, 29),\n",
       "       datetime.date(2017, 12, 30), datetime.date(2017, 12, 31),\n",
       "       datetime.date(2018, 1, 1), datetime.date(2018, 1, 2),\n",
       "       datetime.date(2018, 1, 3), datetime.date(2018, 1, 4),\n",
       "       datetime.date(2018, 1, 5), datetime.date(2018, 1, 6),\n",
       "       datetime.date(2018, 1, 7), datetime.date(2018, 1, 8),\n",
       "       datetime.date(2018, 1, 9), datetime.date(2018, 1, 10),\n",
       "       datetime.date(2018, 1, 11), datetime.date(2018, 1, 12),\n",
       "       datetime.date(2018, 1, 13), datetime.date(2018, 1, 14),\n",
       "       datetime.date(2018, 1, 15), datetime.date(2018, 1, 16),\n",
       "       datetime.date(2018, 1, 17), datetime.date(2018, 1, 18),\n",
       "       datetime.date(2018, 1, 19), datetime.date(2018, 1, 20),\n",
       "       datetime.date(2018, 1, 21), datetime.date(2018, 1, 22),\n",
       "       datetime.date(2018, 1, 23), datetime.date(2018, 1, 24),\n",
       "       datetime.date(2018, 1, 25), datetime.date(2018, 1, 26),\n",
       "       datetime.date(2018, 1, 27), datetime.date(2018, 1, 28),\n",
       "       datetime.date(2018, 1, 29), datetime.date(2018, 1, 30),\n",
       "       datetime.date(2018, 1, 31), datetime.date(2018, 2, 1),\n",
       "       datetime.date(2018, 2, 2), datetime.date(2018, 2, 3),\n",
       "       datetime.date(2018, 2, 4), datetime.date(2018, 2, 5),\n",
       "       datetime.date(2018, 2, 6), datetime.date(2018, 2, 7),\n",
       "       datetime.date(2018, 2, 8), datetime.date(2018, 2, 9),\n",
       "       datetime.date(2018, 2, 10), datetime.date(2018, 2, 11),\n",
       "       datetime.date(2018, 2, 12), datetime.date(2018, 2, 13),\n",
       "       datetime.date(2018, 2, 14), datetime.date(2018, 2, 15),\n",
       "       datetime.date(2018, 2, 16), datetime.date(2018, 2, 17),\n",
       "       datetime.date(2018, 2, 18), datetime.date(2018, 2, 19),\n",
       "       datetime.date(2018, 2, 20), datetime.date(2018, 2, 21),\n",
       "       datetime.date(2018, 2, 22), datetime.date(2018, 2, 23),\n",
       "       datetime.date(2018, 2, 24), datetime.date(2018, 2, 25),\n",
       "       datetime.date(2018, 2, 26), datetime.date(2018, 2, 27),\n",
       "       datetime.date(2018, 2, 28), datetime.date(2018, 3, 1),\n",
       "       datetime.date(2018, 3, 2), datetime.date(2018, 3, 3),\n",
       "       datetime.date(2018, 3, 4), datetime.date(2018, 3, 5),\n",
       "       datetime.date(2018, 3, 6), datetime.date(2018, 3, 7),\n",
       "       datetime.date(2018, 3, 8), datetime.date(2018, 3, 9),\n",
       "       datetime.date(2018, 3, 10), datetime.date(2018, 3, 11),\n",
       "       datetime.date(2018, 3, 12), datetime.date(2018, 3, 13),\n",
       "       datetime.date(2018, 3, 14), datetime.date(2018, 3, 15),\n",
       "       datetime.date(2018, 3, 16), datetime.date(2018, 3, 17),\n",
       "       datetime.date(2018, 3, 18), datetime.date(2018, 3, 19),\n",
       "       datetime.date(2018, 3, 20), datetime.date(2018, 3, 21),\n",
       "       datetime.date(2018, 3, 22), datetime.date(2018, 3, 23),\n",
       "       datetime.date(2018, 3, 24), datetime.date(2018, 3, 25),\n",
       "       datetime.date(2018, 3, 26), datetime.date(2018, 3, 27),\n",
       "       datetime.date(2018, 3, 28), datetime.date(2018, 3, 29),\n",
       "       datetime.date(2018, 3, 30), datetime.date(2018, 3, 31),\n",
       "       datetime.date(2018, 4, 1), datetime.date(2018, 4, 2),\n",
       "       datetime.date(2018, 4, 3), datetime.date(2018, 4, 4),\n",
       "       datetime.date(2018, 4, 5), datetime.date(2018, 4, 6),\n",
       "       datetime.date(2018, 4, 7), datetime.date(2018, 4, 8),\n",
       "       datetime.date(2018, 4, 9), datetime.date(2018, 4, 10),\n",
       "       datetime.date(2018, 4, 11), datetime.date(2018, 4, 12),\n",
       "       datetime.date(2018, 4, 13), datetime.date(2018, 4, 14),\n",
       "       datetime.date(2018, 4, 15), datetime.date(2018, 4, 16),\n",
       "       datetime.date(2018, 4, 17), datetime.date(2018, 4, 18),\n",
       "       datetime.date(2018, 4, 19), datetime.date(2018, 4, 20),\n",
       "       datetime.date(2018, 4, 21), datetime.date(2018, 4, 22),\n",
       "       datetime.date(2018, 4, 23), datetime.date(2018, 4, 24),\n",
       "       datetime.date(2018, 4, 25), datetime.date(2018, 4, 26),\n",
       "       datetime.date(2018, 4, 27), datetime.date(2018, 4, 28),\n",
       "       datetime.date(2018, 4, 29), datetime.date(2018, 4, 30),\n",
       "       datetime.date(2018, 5, 1), datetime.date(2018, 5, 2),\n",
       "       datetime.date(2018, 5, 3), datetime.date(2018, 5, 4),\n",
       "       datetime.date(2018, 5, 5), datetime.date(2018, 5, 6),\n",
       "       datetime.date(2018, 5, 7), datetime.date(2018, 5, 8),\n",
       "       datetime.date(2018, 5, 9), datetime.date(2018, 5, 10),\n",
       "       datetime.date(2018, 5, 11), datetime.date(2018, 5, 12),\n",
       "       datetime.date(2018, 5, 13), datetime.date(2018, 5, 14),\n",
       "       datetime.date(2018, 5, 15), datetime.date(2018, 5, 16),\n",
       "       datetime.date(2018, 5, 17), datetime.date(2018, 5, 18),\n",
       "       datetime.date(2018, 5, 19), datetime.date(2018, 5, 20),\n",
       "       datetime.date(2018, 5, 21), datetime.date(2018, 5, 22),\n",
       "       datetime.date(2018, 5, 23), datetime.date(2018, 5, 24),\n",
       "       datetime.date(2018, 5, 25), datetime.date(2018, 5, 26),\n",
       "       datetime.date(2018, 5, 27), datetime.date(2018, 5, 28),\n",
       "       datetime.date(2018, 5, 29), datetime.date(2018, 5, 30)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T21:21:34.126326Z",
     "start_time": "2018-06-08T21:21:34.114141Z"
    }
   },
   "source": [
    "### Add temperature, sunrise/sunset data\n",
    "\n",
    "Here we use the darksky weather API to store data about the weather on each given day in a local temperature_data.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T20:08:25.079608Z",
     "start_time": "2018-06-11T20:08:25.046426Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_temp_time_dict(data_frame):\n",
    "    base_http = 'https://api.darksky.net/forecast/3245ab0036e72b779cf68adcb07db939/39.833851,-74.871826,'\n",
    "    end_http = '?exclude=currently,flags,alerts,minutely'\n",
    "    hourly_temp_dict = {}\n",
    "    \n",
    "    dates = data_frame['unix_time'].map(pd.Timestamp.date).unique()\n",
    "    \n",
    "    for date in dates:\n",
    "        unix_date = int(time.mktime(date.timetuple())) + 4 * 60 * 60\n",
    "        response = requests.get(base_http + str(unix_date) + end_http) \n",
    "        temp_json = response.json()\n",
    "        hourly_temp_dict[unix_date] = {}\n",
    "        sunset_time = temp_json['daily']['data'][0]['sunsetTime']\n",
    "        sunrise_time = temp_json['daily']['data'][0]['sunriseTime']\n",
    "        for hour_data in temp_json['hourly']['data']:\n",
    "            hourly_temp_dict[unix_date][hour_data['time']] = hour_data\n",
    "            hourly_temp_dict[unix_date][hour_data['time']]['sunset_time'] = sunset_time\n",
    "            hourly_temp_dict[unix_date][hour_data['time']]['sunrise_time'] = sunrise_time\n",
    "        \n",
    "#     count=0\n",
    "#     for index, row in data_frame.iterrows():\n",
    "#         date_obj = date(2017, row['month'], row['day'])\n",
    "#         current_timestamp = int(time.mktime(date_obj.timetuple())) + 4 * 60 * 60\n",
    "\n",
    "#         if not current_timestamp in temp_time_dict.keys():\n",
    "#             r = requests.get(base_http + str(current_timestamp))\n",
    "#             temp_json = r.json()\n",
    "#             temp_time_dict[current_timestamp] = temp_json\n",
    "        \n",
    "    with open('temperature_data_hourly.json', 'w') as outfile:\n",
    "        json.dump(hourly_temp_dict, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T20:09:11.142394Z",
     "start_time": "2018-06-11T20:08:26.577991Z"
    }
   },
   "outputs": [],
   "source": [
    "create_temp_time_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T20:11:11.571268Z",
     "start_time": "2018-06-11T20:11:11.512304Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = None\n",
    "with open('temperature_data_hourly.json') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the json object we just created, we pull the hourly temperature, and determine whether the sun was up or down \n",
    "for each row in the data. We add two rows to the dataframe, 'sun', and 'outside_temperature'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T20:23:10.893361Z",
     "start_time": "2018-06-11T20:23:10.876943Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weather_sunset_columns(row):\n",
    "    #hard-coded year for now. \n",
    "    date_obj = row['unix_time'].date()\n",
    "    #format for API\n",
    "    date_timestamp = int(time.mktime(date_obj.timetuple())) + 4 * 60 * 60\n",
    "    hour_timestamp = date_timestamp + (row['hour'] * 60 * 60)\n",
    "    seconds_timestamp = hour_timestamp + (row['minute'] * 60)\n",
    "    \n",
    "    date_timestamp_string = str(date_timestamp)\n",
    "    hour_timestamp_string = str(hour_timestamp)\n",
    "    \n",
    "    if date_timestamp_string == \"1501905600\" or hour_timestamp_string == \"1501905600\":\n",
    "        import pdb; pdb.set_trace()\n",
    "    temperature_info = json_data[date_timestamp_string][hour_timestamp_string]['temperature']\n",
    "    sunrise = json_data[date_timestamp_string][hour_timestamp_string]['sunrise_time'] #in GMT \n",
    "    sunset = json_data[date_timestamp_string][hour_timestamp_string]['sunset_time']\n",
    "\n",
    "    is_sun_up = 1 if sunrise <= seconds_timestamp <= sunset else 0\n",
    "    return is_sun_up, temperature_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T20:26:19.615245Z",
     "start_time": "2018-06-11T20:23:12.005872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-108-8951758cebbb>(14)create_weather_sunset_columns()\n",
      "-> temperature_info = json_data[date_timestamp_string][hour_timestamp_string]['temperature']\n",
      "(Pdb) date_timestamp_string\n",
      "'1501833600'\n",
      "(Pdb) hour_timestamp_string\n",
      "'1501905600'\n",
      "(Pdb) exit\n",
      "> <ipython-input-108-8951758cebbb>(14)create_weather_sunset_columns()\n",
      "-> temperature_info = json_data[date_timestamp_string][hour_timestamp_string]['temperature']\n",
      "(Pdb) exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "occurred at index 6671",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-6c7f56fa8e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outside_temperature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_weather_sunset_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6002\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6003\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6004\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                                           \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                                           labels=labels)\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-8951758cebbb>\u001b[0m in \u001b[0;36mcreate_weather_sunset_columns\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdate_timestamp_string\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1501905600\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhour_timestamp_string\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1501905600\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtemperature_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhour_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msunrise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhour_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sunrise_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#in GMT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msunset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhour_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sunset_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-8951758cebbb>\u001b[0m in \u001b[0;36mcreate_weather_sunset_columns\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdate_timestamp_string\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1501905600\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhour_timestamp_string\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1501905600\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtemperature_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhour_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msunrise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhour_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sunrise_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#in GMT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msunset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhour_timestamp_string\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sunset_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: occurred at index 6671"
     ]
    }
   ],
   "source": [
    "df['sun'], df['outside_temperature'] = zip(*df.apply(create_weather_sunset_columns, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = save_or_load_from_checkpoint('./checkpoints/data_3.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Encode Columns\n",
    "\n",
    "Convert Categorical to Numerical Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "columns_to_update = [\"device_id_name\", \"event\", \"regression_value_type\"]\n",
    "\n",
    "def encode_columns(data_frame, column_names):\n",
    "    label_encoders = {}\n",
    "    for col in column_names:\n",
    "        values = data_frame[col].unique()\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(values)\n",
    "        label_encoders[col] = le\n",
    "        numerical_values = le.transform(data_frame[col])\n",
    "        data_frame[col] = numerical_values\n",
    "    return data_frame, label_encoders\n",
    "\n",
    "categorical_df = df.copy()\n",
    "# to encode the df we need to encode the NaNs as strings\n",
    "categorical_df = categorical_df.fillna({'regression_value_type':'nan', 'event': 'nan', 'value': 0})\n",
    "categorical_df, label_encoders = encode_columns(categorical_df, columns_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_df.to_hdf('./checkpoints/data_categorical.h5', 'table', mode='w', append=True, complevel=9, complib='zlib', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting with binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_col(data_frame, col_name):\n",
    "    one_hot_encoded = pd.get_dummies(data_frame[col_name], prefix=col_name)\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_df = df.copy()\n",
    "binary_df = binary_df.drop('regression_value_type', axis=1)\n",
    "binary_df = binary_df.drop('value', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_encoded = one_hot_encode_col(binary_df, 'event')\n",
    "binary_df = pd.concat([binary_df, event_encoded], axis=1)\n",
    "binary_df = binary_df.drop('event', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_df.to_hdf('./checkpoints/data_binary_encoded.h5', 'table', mode='w', append=True, complevel=9, complib='zlib', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
